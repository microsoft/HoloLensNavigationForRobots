{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics HoloLens ROS Navigation System The HoloLensNavigation system shows how a HoloLens device can be placed on the head of Pepper robot and provide a self-calibrating indoor navigation solution. How it Works System Diagram Sample Code Modules: HoloLensSpatialMapping Universal Windows Platform (UWP) application solution for HoloLens. It contains two projects: - HololensSpatialMapping: Uses device sensors to capture and maintain 3D map of local environment - HoloLensBridge: Communicates with HoloROSBridge. HoloLens_Localization HoloLens_Localization is a ROS (Melodic) package that computes the local position of the robot based on sensor measurements as the robot moves through calibrated poses and navigates through the environment. ROS package including HoloLens Localization module, offline calibration between HoloLens and Robot's head, and online calibration between HoloLens and Robot's base. HoloROSBridge HoloROSBridge is a ROS (Melodic) package that communicates with the HoloLensSpatialMapping application running on the HoloLens device. ROS package of HoloLens brigde. Module for using HoloLens in ROS system. holo_nav_dash holo_nav_dash is a ROS (Melodic) package that provides a local http server and a browser-based operational interface for starting up and monitoring calibration and navigation operations. navigation_launcher navigation_launcher is a ROS (Melodic) package that contains launch scripts for starting up components for the HoloLens stack, the HoloLens Navigation stack, and the ROS Navigation stack. Modes of Operation The system operates in one of three modes: map generation, position calibration, and navigation. Map Capture and Generation Mode Map Capture theory of operation. Position Calibration Mode Calibration Mode theory of operation. Navigation Mode Navigation Mode theory of operation. Prerequisites, Installation and Build Follow these links for instructions in preparing the system: Setup Instructions Map Generation Instructions HoloLens Mounting and Pepper Configuration Instructions Calibration and Navigation Operations The following procedure assumes that a 2D floor map has already been generated and installed : Launch System and Perform Calibration HoloLens Stack (HoloLens) Boot HoloLens Bridge Launch the HoloLensNavigation application from Device Portal (access the HoloLens ip from browser). Or use alternative methods. (ROS) Launch Pepper's stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface The local ROS computer's network interface name can be found using the terminal command \"ifconfig\" and looking for the name associated with the active IP address. Do not include the colon after the network interface. Ideally start Pepper with life disabled. Use Choregraph or refer to the tips document for alternative options. (ROS) Launch HoloLens stack $ roslaunch navigation_launcher hololensstack.launch HoloLens_ip:= hololens ip Note that XTerm needs to be installed for this as the script uses it to interact with the calibration. Navigation Stack (ROS) Launch Navigation program roslaunch navigation_launcher navstack.launch Calibration If desired, launch the Dashboard UI using these instructions . Alternatively, use the console UI in the calibration window and use either Choregraph or connect to Pepper via SSH and set the pitch directly: move Pepper's head into inital/default pose. Use qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.3 press space to record the initial position. move Pepper's head upward. Use either Choregraph or connect to Pepper via SSH and set the pitch directly: qicli call ALMotion.setAngles \"HeadPitch\" -0.35 0.3 press space again to record the new position. reset Pepper's head pitch and then rotate to left. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.7 0.3 press space to record the new position. rotate Pepper's head to the right. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadYaw\" -0.7 0.3 press space to record the new position. press c to calibrate. reset Pepper's head pitch and rotation. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.3 Navigation Operations (ROS) Launch rviz $ rosrun rviz rviz add Map and Pepper RobotModel topics. Alternatively, load the pepper.rviz rviz configuration file. In rviz, select 2D Pose Estimate and set Pepper's inital position and direction on the map. Try to be as precise as possible. The script will calculate a pose estimate and localize the Pepper model. In rviz, select 2D Nav Goal and select a destination goal and orientation on the map. Pepper navigation will start. Running Process (INDIVIDUAL NODES) Pepper ROS full stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface example: roslaunch pepper_bringup pepper_full.launch nao_ip:=10.1.1.202 network_interface:=enp3s0 HoloLens ROS Bridge $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens_ip 1234 example: rosrun hololens_ros_bridge hololens_ros_bridge_node 10.1.1.206 1234 ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml HoloLens Anchor Localizer $ rosrun hololens_localizer anchor_localizer Localizer Calibration $ rosrun hololens_localizer static_calibration robot odom frame robot head frame robot base link [calibrationFileName] example: rosrun hololens_localizer static_calibration odom Head base_footprint calibrationData.bin Dynamic Adjuster $ rosrun hololens_localizer dynamic_adjuster.py robot foot frame example: rosrun hololens_localizer dynamic_adjuster.py","title":"Home"},{"location":"#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"#hololens-ros-navigation-system","text":"The HoloLensNavigation system shows how a HoloLens device can be placed on the head of Pepper robot and provide a self-calibrating indoor navigation solution.","title":"HoloLens ROS Navigation System"},{"location":"#how-it-works","text":"","title":"How it Works"},{"location":"#system-diagram","text":"","title":"System Diagram"},{"location":"#sample-code-modules","text":"","title":"Sample Code Modules:"},{"location":"#hololensspatialmapping","text":"Universal Windows Platform (UWP) application solution for HoloLens. It contains two projects: - HololensSpatialMapping: Uses device sensors to capture and maintain 3D map of local environment - HoloLensBridge: Communicates with HoloROSBridge.","title":"HoloLensSpatialMapping"},{"location":"#hololens_localization","text":"HoloLens_Localization is a ROS (Melodic) package that computes the local position of the robot based on sensor measurements as the robot moves through calibrated poses and navigates through the environment. ROS package including HoloLens Localization module, offline calibration between HoloLens and Robot's head, and online calibration between HoloLens and Robot's base.","title":"HoloLens_Localization"},{"location":"#holorosbridge","text":"HoloROSBridge is a ROS (Melodic) package that communicates with the HoloLensSpatialMapping application running on the HoloLens device. ROS package of HoloLens brigde. Module for using HoloLens in ROS system.","title":"HoloROSBridge"},{"location":"#holo_nav_dash","text":"holo_nav_dash is a ROS (Melodic) package that provides a local http server and a browser-based operational interface for starting up and monitoring calibration and navigation operations.","title":"holo_nav_dash"},{"location":"#navigation_launcher","text":"navigation_launcher is a ROS (Melodic) package that contains launch scripts for starting up components for the HoloLens stack, the HoloLens Navigation stack, and the ROS Navigation stack.","title":"navigation_launcher"},{"location":"#modes-of-operation","text":"The system operates in one of three modes: map generation, position calibration, and navigation.","title":"Modes of Operation"},{"location":"#map-capture-and-generation-mode","text":"Map Capture theory of operation.","title":"Map Capture and Generation Mode"},{"location":"#position-calibration-mode","text":"Calibration Mode theory of operation.","title":"Position Calibration Mode"},{"location":"#navigation-mode","text":"Navigation Mode theory of operation.","title":"Navigation Mode"},{"location":"#prerequisites-installation-and-build","text":"Follow these links for instructions in preparing the system:","title":"Prerequisites, Installation and Build"},{"location":"#setup-instructions","text":"","title":"Setup Instructions"},{"location":"#map-generation-instructions","text":"","title":"Map Generation Instructions"},{"location":"#hololens-mounting-and-pepper-configuration-instructions","text":"","title":"HoloLens Mounting and Pepper Configuration Instructions"},{"location":"#calibration-and-navigation-operations","text":"The following procedure assumes that a 2D floor map has already been generated and installed :","title":"Calibration and Navigation Operations"},{"location":"#launch-system-and-perform-calibration","text":"","title":"Launch System and Perform Calibration"},{"location":"#hololens-stack","text":"(HoloLens) Boot HoloLens Bridge Launch the HoloLensNavigation application from Device Portal (access the HoloLens ip from browser). Or use alternative methods. (ROS) Launch Pepper's stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface The local ROS computer's network interface name can be found using the terminal command \"ifconfig\" and looking for the name associated with the active IP address. Do not include the colon after the network interface. Ideally start Pepper with life disabled. Use Choregraph or refer to the tips document for alternative options. (ROS) Launch HoloLens stack $ roslaunch navigation_launcher hololensstack.launch HoloLens_ip:= hololens ip Note that XTerm needs to be installed for this as the script uses it to interact with the calibration.","title":"HoloLens Stack"},{"location":"#navigation-stack","text":"(ROS) Launch Navigation program roslaunch navigation_launcher navstack.launch","title":"Navigation Stack"},{"location":"#calibration","text":"If desired, launch the Dashboard UI using these instructions . Alternatively, use the console UI in the calibration window and use either Choregraph or connect to Pepper via SSH and set the pitch directly: move Pepper's head into inital/default pose. Use qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.3 press space to record the initial position. move Pepper's head upward. Use either Choregraph or connect to Pepper via SSH and set the pitch directly: qicli call ALMotion.setAngles \"HeadPitch\" -0.35 0.3 press space again to record the new position. reset Pepper's head pitch and then rotate to left. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.7 0.3 press space to record the new position. rotate Pepper's head to the right. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadYaw\" -0.7 0.3 press space to record the new position. press c to calibrate. reset Pepper's head pitch and rotation. Use either Choregraph or connect to Pepper via SSH: qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.3 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.3","title":"Calibration"},{"location":"#navigation-operations","text":"(ROS) Launch rviz $ rosrun rviz rviz add Map and Pepper RobotModel topics. Alternatively, load the pepper.rviz rviz configuration file. In rviz, select 2D Pose Estimate and set Pepper's inital position and direction on the map. Try to be as precise as possible. The script will calculate a pose estimate and localize the Pepper model. In rviz, select 2D Nav Goal and select a destination goal and orientation on the map. Pepper navigation will start.","title":"Navigation Operations"},{"location":"#running-process-individual-nodes","text":"Pepper ROS full stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface example: roslaunch pepper_bringup pepper_full.launch nao_ip:=10.1.1.202 network_interface:=enp3s0 HoloLens ROS Bridge $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens_ip 1234 example: rosrun hololens_ros_bridge hololens_ros_bridge_node 10.1.1.206 1234 ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml HoloLens Anchor Localizer $ rosrun hololens_localizer anchor_localizer Localizer Calibration $ rosrun hololens_localizer static_calibration robot odom frame robot head frame robot base link [calibrationFileName] example: rosrun hololens_localizer static_calibration odom Head base_footprint calibrationData.bin Dynamic Adjuster $ rosrun hololens_localizer dynamic_adjuster.py robot foot frame example: rosrun hololens_localizer dynamic_adjuster.py","title":"Running Process (INDIVIDUAL NODES)"},{"location":"CODE_OF_CONDUCT/","text":"Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"CODE_OF_CONDUCT/#microsoft-open-source-code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"SECURITY/","text":"Security Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below. Reporting Security Issues Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs. Preferred Languages We prefer all communications to be in English. Policy Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"SECURITY"},{"location":"SECURITY/#security","text":"Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below.","title":"Security"},{"location":"SECURITY/#reporting-security-issues","text":"Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs.","title":"Reporting Security Issues"},{"location":"SECURITY/#preferred-languages","text":"We prefer all communications to be in English.","title":"Preferred Languages"},{"location":"SECURITY/#policy","text":"Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"Policy"},{"location":"SUPPORT/","text":"TODO: The maintainer of this repo has not yet edited this file REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo. Support How to file issues and get help This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? . Microsoft Support Policy Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#todo-the-maintainer-of-this-repo-has-not-yet-edited-this-file","text":"REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#support","text":"","title":"Support"},{"location":"SUPPORT/#how-to-file-issues-and-get-help","text":"This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? .","title":"How to file issues and get help"},{"location":"SUPPORT/#microsoft-support-policy","text":"Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"Microsoft Support Policy"},{"location":"Setup/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Prerequisites, Installation and Build Instructions Prerequisites Ubuntu 18.04.5 LTS Install v18 LTS from https://releases.ubuntu.com/18.04/. Install XTerm ROS Melodic http://wiki.ros.org/melodic/Installation/Ubuntu. Additional ROS packages required: $ sudo apt-get install ros-melodic-driver-base $ sudo apt-get install ros-melodic-move-base-msgs ros-melodic-octomap ros-melodic-octomap-msgs $ sudo apt-get install ros-melodic-map-server $ sudo apt-get install ros-melodic-camera-info-manager ros-melodic-camera-info-manager-py $ sudo apt-get install ros-melodic-rgbd-launch $ sudo apt-get install ros-melodic-husky-navigation $ sudo apt-get install python-catkin-tools Optional: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-teleop/teleop_twist_keyboard $ cd ~/catkin_ws/ $ catkin_make Ceres Solver Install dependecies: sudo apt-get install libgoogle-glog-dev sudo apt-get install libatlas-base-dev sudo apt-get install libeigen3-dev Get Ceres Solver source code, build and install it: $ mkdir -p ~/ceres $ cd ~/ceres/ wget http://ceres-solver.org/ceres-solver-1.14.0.tar.gz tar xvf ceres-solver-1.14.0.tar.gz mkdir ceres-build cd ceres-build cmake ../ceres-solver-1.14.0 make -j3 sudo make install Pepper http://wiki.ros.org/pepper $ sudo apt-get install ros-melodic-pepper-.* $ sudo apt install ros-melodic-naoqi-bridge-msgs ros-melodic-naoqi-libqi ros-melodic-naoqi-driver ros-melodic-naoqi-libqicore Download python SDK from https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux. Extract pynaoqi-python2.7-2.5.7.1-linux64.tar.gz to ~/nao Add naoqi python pythonSDK path to .bashrc: $ echo export PYTHONPATH=${PYTHONPATH}:~/nao/pynaoqi-python2.7-2.5.7.1-linux64/lib/python2.7/site-packages ~/.bashrc Source packages: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-naoqi/naoqi_dcm_driver $ git clone https://github.com/ros-naoqi/naoqi_bridge $ git clone https://github.com/ros-naoqi/pepper_robot $ cd ~/catkin_ws/ $ catkin_make Disable audio in boot configuration file (set flag in line 85 from true to false): $ sudo gedit /opt/ros/melodic/share/naoqi_driver/share/boot_config.json Optional: Install Choregraph. If Choregraph fails to start, try: sudo ln -sf /usr/lib/x86_64-linux-gnu/libz.so /opt/'Softbank Robotics'/'Choregraphe Suite 2.5'/lib/libz.so.1 Hololens Navigation Installation Copy HoloROSBridge to ~/catkin_ws/src/HoloROSBridge Copy HoloLens_Localization to ~/catkin_ws/src/HoloLens_Localization Copy navigation_launcher to ~/catkin_ws/src/navigation_launcher $ cd ~/catkin_ws/src/HoloLens_Localization/scripts $ chown $USER:$USER dynamic_adjuster.py $ chmod +x dynamic_adjuster.py $ chown $USER:$USER localizer.py $ chmod +x localizer.py Build $ cd ~/catkin_ws/ $ catkin_make HoloLens Spatial Mapping application (Windows) Enable development mode on HoloLens. Pair device with PC. Enable Device Portal. Install Visual Studio 2019 with Universal Windows Platform build environment. After loading the solution in Visual Studio for the first time, navigate to Tools - NuGet Package Manager - Package Manager Console . If you see a message \"Some NuGet packages are missing from this solution...\", click on Restore to download and install the missing Eigen package. At the time of development, v3.3.3 of Eigen was used for this solution. For HoloLens generation 1: build with \"Solution Configure:Release\", \"Solution Platform: x86\" For HoloLens generation 2: build with \"Solution Configure:Release\", \"Solution Platform: ARM64\" If HoloLens device is connected via USB: Deploy target: \"Device\" If HoloLens is connected via WiFi, deploy target using Remote Machine settings. Deploy information: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-visual-studio After launching the application, you can air-tap to toggle between wirefreame and solid model render mode.","title":"Home"},{"location":"Setup/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/#prerequisites-installation-and-build-instructions","text":"","title":"Prerequisites, Installation and Build Instructions"},{"location":"Setup/#prerequisites","text":"","title":"Prerequisites"},{"location":"Setup/#ubuntu-18045-lts","text":"Install v18 LTS from https://releases.ubuntu.com/18.04/. Install XTerm","title":"Ubuntu 18.04.5 LTS"},{"location":"Setup/#ros-melodic","text":"http://wiki.ros.org/melodic/Installation/Ubuntu. Additional ROS packages required: $ sudo apt-get install ros-melodic-driver-base $ sudo apt-get install ros-melodic-move-base-msgs ros-melodic-octomap ros-melodic-octomap-msgs $ sudo apt-get install ros-melodic-map-server $ sudo apt-get install ros-melodic-camera-info-manager ros-melodic-camera-info-manager-py $ sudo apt-get install ros-melodic-rgbd-launch $ sudo apt-get install ros-melodic-husky-navigation $ sudo apt-get install python-catkin-tools Optional: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-teleop/teleop_twist_keyboard $ cd ~/catkin_ws/ $ catkin_make","title":"ROS Melodic"},{"location":"Setup/#ceres-solver","text":"Install dependecies: sudo apt-get install libgoogle-glog-dev sudo apt-get install libatlas-base-dev sudo apt-get install libeigen3-dev Get Ceres Solver source code, build and install it: $ mkdir -p ~/ceres $ cd ~/ceres/ wget http://ceres-solver.org/ceres-solver-1.14.0.tar.gz tar xvf ceres-solver-1.14.0.tar.gz mkdir ceres-build cd ceres-build cmake ../ceres-solver-1.14.0 make -j3 sudo make install","title":"Ceres Solver"},{"location":"Setup/#pepper","text":"http://wiki.ros.org/pepper $ sudo apt-get install ros-melodic-pepper-.* $ sudo apt install ros-melodic-naoqi-bridge-msgs ros-melodic-naoqi-libqi ros-melodic-naoqi-driver ros-melodic-naoqi-libqicore Download python SDK from https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux. Extract pynaoqi-python2.7-2.5.7.1-linux64.tar.gz to ~/nao Add naoqi python pythonSDK path to .bashrc: $ echo export PYTHONPATH=${PYTHONPATH}:~/nao/pynaoqi-python2.7-2.5.7.1-linux64/lib/python2.7/site-packages ~/.bashrc Source packages: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-naoqi/naoqi_dcm_driver $ git clone https://github.com/ros-naoqi/naoqi_bridge $ git clone https://github.com/ros-naoqi/pepper_robot $ cd ~/catkin_ws/ $ catkin_make Disable audio in boot configuration file (set flag in line 85 from true to false): $ sudo gedit /opt/ros/melodic/share/naoqi_driver/share/boot_config.json Optional: Install Choregraph. If Choregraph fails to start, try: sudo ln -sf /usr/lib/x86_64-linux-gnu/libz.so /opt/'Softbank Robotics'/'Choregraphe Suite 2.5'/lib/libz.so.1","title":"Pepper"},{"location":"Setup/#hololens-navigation-installation","text":"Copy HoloROSBridge to ~/catkin_ws/src/HoloROSBridge Copy HoloLens_Localization to ~/catkin_ws/src/HoloLens_Localization Copy navigation_launcher to ~/catkin_ws/src/navigation_launcher $ cd ~/catkin_ws/src/HoloLens_Localization/scripts $ chown $USER:$USER dynamic_adjuster.py $ chmod +x dynamic_adjuster.py $ chown $USER:$USER localizer.py $ chmod +x localizer.py","title":"Hololens Navigation Installation"},{"location":"Setup/#build","text":"$ cd ~/catkin_ws/ $ catkin_make","title":"Build"},{"location":"Setup/#hololens-spatial-mapping-application-windows","text":"Enable development mode on HoloLens. Pair device with PC. Enable Device Portal. Install Visual Studio 2019 with Universal Windows Platform build environment. After loading the solution in Visual Studio for the first time, navigate to Tools - NuGet Package Manager - Package Manager Console . If you see a message \"Some NuGet packages are missing from this solution...\", click on Restore to download and install the missing Eigen package. At the time of development, v3.3.3 of Eigen was used for this solution. For HoloLens generation 1: build with \"Solution Configure:Release\", \"Solution Platform: x86\" For HoloLens generation 2: build with \"Solution Configure:Release\", \"Solution Platform: ARM64\" If HoloLens device is connected via USB: Deploy target: \"Device\" If HoloLens is connected via WiFi, deploy target using Remote Machine settings. Deploy information: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-visual-studio After launching the application, you can air-tap to toggle between wirefreame and solid model render mode.","title":"HoloLens Spatial Mapping application (Windows)"},{"location":"Setup/MAP/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Map Generation Instructions HoloLens Spatial Mapping Use the HoloLens device to build a spatial mapping of your environment. Map the floor up to at least your eye level, and make sure to carefully trace along the floor edges to get accurate readings. For visual feedback, compile and run the HoloLensNavigation application and complete the spatial mesh mapping. Alternatively, compile and run the lighter weight Microsoft's Holographic spatial mapping sample found here . Create a floor plan image from HoloLens' Spatial Map in ROS launch the HoloLensNavigation app on your HoloLens device there are different ways to launch the application. Easiest way is to use the Device Portal to launch application. Alternatively wear the headset and launch from GUI, or launch from VisualStudio. launch the ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml launch the HoloLens bridge in ROS $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens ip 1234 note that 1234 is the port number. launch the HoloLens localizer in ROS $ rosrun hololens_localizer anchor_localizer launch the image saver node in ROS $ rosrun image_view image_saver image:=/hololens/image launch rviz in ROS $ rviz select \"2D Pose Estimate\" and then click anywhere on the map view to set initial position in any location and direction. This instructs image_saver to create a cross-section of the HoloLens' spatial map and save it as left000.jpg in the same folder the image_saver was launched. open the file left0000.jpg open the file left0000.jpg with your favorite image editor. using the depicted pointcloud outline, create a ROS compliant image map. save the modified left0000.jpg file in the navigation_launcher/params/ folder.","title":"MAP"},{"location":"Setup/MAP/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/MAP/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/MAP/#map-generation-instructions","text":"","title":"Map Generation Instructions"},{"location":"Setup/MAP/#hololens-spatial-mapping","text":"Use the HoloLens device to build a spatial mapping of your environment. Map the floor up to at least your eye level, and make sure to carefully trace along the floor edges to get accurate readings. For visual feedback, compile and run the HoloLensNavigation application and complete the spatial mesh mapping. Alternatively, compile and run the lighter weight Microsoft's Holographic spatial mapping sample found here .","title":"HoloLens Spatial Mapping"},{"location":"Setup/MAP/#create-a-floor-plan-image-from-hololens-spatial-map-in-ros","text":"launch the HoloLensNavigation app on your HoloLens device there are different ways to launch the application. Easiest way is to use the Device Portal to launch application. Alternatively wear the headset and launch from GUI, or launch from VisualStudio. launch the ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml launch the HoloLens bridge in ROS $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens ip 1234 note that 1234 is the port number. launch the HoloLens localizer in ROS $ rosrun hololens_localizer anchor_localizer launch the image saver node in ROS $ rosrun image_view image_saver image:=/hololens/image launch rviz in ROS $ rviz select \"2D Pose Estimate\" and then click anywhere on the map view to set initial position in any location and direction. This instructs image_saver to create a cross-section of the HoloLens' spatial map and save it as left000.jpg in the same folder the image_saver was launched. open the file left0000.jpg open the file left0000.jpg with your favorite image editor. using the depicted pointcloud outline, create a ROS compliant image map. save the modified left0000.jpg file in the navigation_launcher/params/ folder.","title":"Create a floor plan image from HoloLens' Spatial Map in ROS"},{"location":"Setup/MountHololens/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics HoloLens Mounting and Pepper Configuration Instructions HoloLens Device Portal Enable Device Portal on the HoloLens device in Settings- For developers Navigate to the HoloLens IP using your web browser and set up a user/password. Certificate Installation Navigate to the HoloLens IP using your web browser and download the HoloLens certificate. Convert and install it: $ sudo apt-get install ca-certificates -y $ openssl x509 -outform der -in certificate.pem -out certificate.crt $ sudo cp certificate.crt /usr/local/share/ca-certificates $ sudo update-ca-certificates Change or disable HoloLens sleep settings Navigate to the HoloLens IP using your web browser and log in with the user/pwd set above. In System- Preferences set sleep settings. To disable, use the browser's inspect feature and add 0 as a list option, then select 0 . Note you will need to do this twice if you want to disable sleep for both battery and plugged-in settings. Pepper Configuration Pepper qicli commands can be issued via the Choregraphe application or directly using a secure shell. Start Pepper with autonomous life disabled Disable Pepper autonomous life mode using Choregraph: Connect to Pepper using Choregraph. Click on blue heart icon in upper right corner. Wake Pepper up by clicking on sunshine icon in upper right corner Disable Pepper autonomous life mode using ssh : $ ssh nao@ pepper IP nao stop naoqi-bin --disable-life Connect to Pepper again and: $ ssh nao@ pepper IP qicli call ALMotion.wakeUp To make Pepper go to sleep: $ ssh nao@ pepper IP qicli call ALMotion.rest To shut down Pepper: $ ssh nao@ pepper IP sudo shutdown -h now To get the joint names for the body or a chain: $ ssh nao@ pepper IP qicli call ALMotion.getBodyNames Body To view Pepper's current joint state: $ ssh nao@ pepper IP qicli call ALMotion.getSummary To change Pepper's head pitch: $ ssh nao@ pepper IP qicli call ALMotion.setAngles HeadPitch 0.0 0.3 The setAngles call is a non-blocking call. Parameters: - names \u2013 The name or names of joints, chains, \u201cBody\u201d, \u201cJointActuators\u201d, \u201cJoints\u201d or \u201cActuators\u201d. - angles \u2013 One or more angles in radians - fractionMaxSpeed \u2013 The fraction of maximum speed to use Valid Pepper joint names can be found here and here , or call getBodyNames for a complete list. Pepper RViz configuration file location: /opt/ros/melodic/share/naoqi_driver/share/pepper.rviz Miscellaneous Enumerate network interfaces on Ubuntu: ip l show ip a show eno1","title":"MountHololens"},{"location":"Setup/MountHololens/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/MountHololens/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/MountHololens/#hololens-mounting-and-pepper-configuration-instructions","text":"","title":"HoloLens Mounting and Pepper Configuration Instructions"},{"location":"Setup/MountHololens/#hololens-device-portal","text":"Enable Device Portal on the HoloLens device in Settings- For developers Navigate to the HoloLens IP using your web browser and set up a user/password.","title":"HoloLens Device Portal"},{"location":"Setup/MountHololens/#certificate-installation","text":"Navigate to the HoloLens IP using your web browser and download the HoloLens certificate. Convert and install it: $ sudo apt-get install ca-certificates -y $ openssl x509 -outform der -in certificate.pem -out certificate.crt $ sudo cp certificate.crt /usr/local/share/ca-certificates $ sudo update-ca-certificates","title":"Certificate Installation"},{"location":"Setup/MountHololens/#change-or-disable-hololens-sleep-settings","text":"Navigate to the HoloLens IP using your web browser and log in with the user/pwd set above. In System- Preferences set sleep settings. To disable, use the browser's inspect feature and add 0 as a list option, then select 0 . Note you will need to do this twice if you want to disable sleep for both battery and plugged-in settings.","title":"Change or disable HoloLens sleep settings"},{"location":"Setup/MountHololens/#pepper-configuration","text":"Pepper qicli commands can be issued via the Choregraphe application or directly using a secure shell.","title":"Pepper Configuration"},{"location":"Setup/MountHololens/#start-pepper-with-autonomous-life-disabled","text":"Disable Pepper autonomous life mode using Choregraph: Connect to Pepper using Choregraph. Click on blue heart icon in upper right corner. Wake Pepper up by clicking on sunshine icon in upper right corner Disable Pepper autonomous life mode using ssh : $ ssh nao@ pepper IP nao stop naoqi-bin --disable-life Connect to Pepper again and: $ ssh nao@ pepper IP qicli call ALMotion.wakeUp To make Pepper go to sleep: $ ssh nao@ pepper IP qicli call ALMotion.rest To shut down Pepper: $ ssh nao@ pepper IP sudo shutdown -h now To get the joint names for the body or a chain: $ ssh nao@ pepper IP qicli call ALMotion.getBodyNames Body To view Pepper's current joint state: $ ssh nao@ pepper IP qicli call ALMotion.getSummary To change Pepper's head pitch: $ ssh nao@ pepper IP qicli call ALMotion.setAngles HeadPitch 0.0 0.3 The setAngles call is a non-blocking call. Parameters: - names \u2013 The name or names of joints, chains, \u201cBody\u201d, \u201cJointActuators\u201d, \u201cJoints\u201d or \u201cActuators\u201d. - angles \u2013 One or more angles in radians - fractionMaxSpeed \u2013 The fraction of maximum speed to use Valid Pepper joint names can be found here and here , or call getBodyNames for a complete list.","title":"Start Pepper with autonomous life disabled"},{"location":"Setup/MountHololens/#pepper-rviz-configuration-file","text":"location: /opt/ros/melodic/share/naoqi_driver/share/pepper.rviz","title":"Pepper RViz configuration file"},{"location":"Setup/MountHololens/#miscellaneous","text":"Enumerate network interfaces on Ubuntu: ip l show ip a show eno1","title":"Miscellaneous"},{"location":"linux/holo_nav_dash/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics HoloNavDash HoloLens Navigation Dashboard for ROS Launch Instructions Run. rosrun holo_nav_dash holo_nav_dash.py With custom values. rosrun holo_nav_dash holo_nav_dash.py _hnd_port:=8000 Publishing to a different topic (in this case my_cmd_vel ). rosrun holo_nav_dash holo_nav_dash.py cmd_vel:=my_cmd_vel Operations Open your favorite webbrowser and navigate to http://localhost:8000","title":"Home"},{"location":"linux/holo_nav_dash/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"linux/holo_nav_dash/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"linux/holo_nav_dash/#holonavdash","text":"HoloLens Navigation Dashboard for ROS","title":"HoloNavDash"},{"location":"linux/holo_nav_dash/#launch-instructions","text":"Run. rosrun holo_nav_dash holo_nav_dash.py With custom values. rosrun holo_nav_dash holo_nav_dash.py _hnd_port:=8000 Publishing to a different topic (in this case my_cmd_vel ). rosrun holo_nav_dash holo_nav_dash.py cmd_vel:=my_cmd_vel","title":"Launch Instructions"},{"location":"linux/holo_nav_dash/#operations","text":"Open your favorite webbrowser and navigate to http://localhost:8000","title":"Operations"}]}