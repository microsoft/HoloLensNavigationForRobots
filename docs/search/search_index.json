{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics HoloLens Navigation for Robots Welcome! The example system in this repository shows how a HoloLens device can be placed on the head of Pepper robot and provide it with a self-calibrating indoor navigation solution. The calibration process is described in detail in the paper: Dynamic Calibration between a Mobile Robot and SLAM Device for Navigation and is demonstrated in the video below (click image to download video) : Who might be interested in using this repository? The HoloLens Navigation For Robots project is an example system that exposes key technology challenges in helping robots get where they need to be. Its purpose is to support research and education in the field of Indoor Robot Navigation with an instruction guide for building a sample navigation software system that runs on off-the-shelf hardware components and lends itself to modifications and experimentation. Its function is to measure environmental parameters with 3D spatial sensors to compute and calibrate the position of a robot's mobile base relative to a HoloLens device attached to its head to enable the use of the generic ROS indoor navigation solution for service robots. The designation \"service-robot\" refers to a requirement that it is designed to safely operate in the midst of humans, as opposed to an \"industrial-robot\" that usually repeats tasks within a cordoned-off safety zone. The HoloLens Navigation for Robots repository was adapted from a project originally created by interns working for Microsoft Research Asia. We've built on their work and put together this repository and instruction set to help students learn with a fun hand's-on project that provides exposure to key technologies used in service-robotics. Researchers in the field of autonomous navigation can use the repository to support experimentation in the use of intelligent solutions for route-planning and hazard-avoidance. No matter where your interest lies, we hope you find the HoloLens Navigation for Robots repository a fun and easy way to work and learn in the field of robotics. How It Works The system is comprised of three primary hardware components that run the software code modules and navigation operations: a Pepper robot, a HoloLens device, networked together with and bridged to the ROS navigation solution running on a Navigation PC. Additionally, a Build PC running Windows 10 is required to compile and deploy this project's sample application onto the HoloLens device. The system operates in one of three modes: - Map Capture - Calibration - Navigation In the map capture mode, the system takes the 3D environment captured by the HoloLens device and converts it into a 2D map image compatible with the built-in ROS navigation solution. In the calibration mode, the system animates the robot and calculates the position of the HoloLens device mounted on the Pepper robot's head relative to the position of the robot's base, which contains the motorized wheels. In the navigation mode, the system uses the dynamic position of the HoloLens device in the navigation space to indicate the position of the robot's mobile base, which is driven by the ROS navigation solution based on plans computed in response to goal points indicated by the user on a 2D map. Hardware Devices: HoloLens (ver. 1 or 2) - battery-powered mobile computer hosting camera and depth sensors used for map capture and localization Navigation PC (Ubuntu 18.04) - x64 PC hosting ROS (Melodic) navigation solution, calibration, and operational scripts Pepper robot - battery-powered semi-humanoid robot capable of indoor locomotion and independent movement of head and body parts Build PC (Windows 10) - x64 PC hosting Visual Studio 2019 solution for building and deploying the HoloLensNavigation UWP app on the HoloLens device Sample Software Modules: HoloLensSpatialMapping - Universal Windows Platform (UWP) application solution for HoloLens. It contains two projects: HololensSpatialMapping - Uses device sensors to capture and maintain 3D map of local environment HoloLensBridge - Communication link with ROS system running on the PC hololens_localizer - Custom ROS (Melodic) package that computes the local position of the robot base using sensor measurements as the robot moves through calibrated poses and navigates through the environment. During offline calibration, it computes the relative positions of the HoloLens device on the robot's head and the robot's base static_calibration - node calculates the transform offset between the HoloLens device and the Pepper robot's base and stores it in the calibration file dynamic_adjuster - node that monitors the neck joint positions of the robot and adjusts the transform offset between the HoloLens device and the robot base as needed. anchor_localizer - node that provides the calculated dynamic position of the robot mobile base relative to the 2D navigation map hololens_ros_bridge - Custom ROS (Melodic) package providing IP communication between the HoloLens device and the ROS system holo_nav_dash - Custom ROS (Melodic) package providing a local http server and a browser-based operational interface for starting up and monitoring calibration and navigation operations navigation_launcher - Custom ROS (Melodic) package that contains launch scripts for starting up components for the HoloLens stack, the HoloLens Navigation stack, and the ROS Navigation stack Other Software Modules map_server - built-in ROS node that stores and serves data requests from the map asset image_view - built-in ROS node for viewing image topics and provides the image_saver tool for capturing a graphic image file of the map rviz - built-in ROS node for navigation operations move_base - built-in ROS node providing an implementation of an action that, given a pose (position) goal on the map using rviz, will attempt to reach it by sending movement commands to the robot's mobile base global_planner - built-in ROS node that give a current position and a goal postion on a 2D map, calculates a route through obstacles indicated on the map base_local_planner - built-in ROS node calculates the motor control commands to send to the base in order to follow a route calculated by the global planner Pepper Naoqi Stack - Pepper SDK software providing IP communication nodes between the PC and the Pepper robot, as well as a pose_manager node that interprets movement requests into hardware joint motor commands Modes of Operation The system operates in one of three modes: map generation, position calibration, and navigation. Map Capture and Generation Mode The purpose of this mode is to create a 2D floorplan of the navigable indoor space. First, the HoloLensNavigation application is run on the HoloLens device while the user scans the indoor space to be navigated by methodically moving it throughout the location. The camera and depth sensors in the device are used to capture and store a 3D spatial map of the room and adjacent areas. After capture, the 3D spatial map must be converted to the 2D floor-plan map form used by the ROS navigation solution. The conversion is accomplished by first launching the hololens_ros_bridge, hololens_localizer, map_server, image_view, and rviz ROS nodes on the PC. The point cloud from the HoloLens spatial map is made available to the ROS system through the hololens_ros_bridge. After the ROS nodes are running, rviz is instructed to use the image_view node and the \"2D Pose Estimate\" process to create a cross-section of the HoloLens' spatial map six inches above and parallel to the floor plane and then save it as a 2D point-cloud image in a JPG file. Using any image editing application, the 2D point cloud is then manually cleaned up to produce a ROS-compliant floorplan navigation map image. Position Calibration Mode The purpose of this mode is to create a static calibration file that defines the relative positions of the Hololens device mounted on the Pepper robot's head and the robot's base containing movement wheels and motors. The HoloLensNavigation app connects to the hololens_ros_bridge node running on the Operations PC and provides spatial anchors representing the current position of the HoloLens device within the 3D map space. On the PC, the Dashboard UI is launched with the holo_nav_dash ROS node and a local http server providing access to data from other ROS nodes as well as Naoqi protocol commands that are sent to the Pepper robot. Buttons are provided in the UI that command the Pepper head to move to specific positions and record sensor readings for use in calculating a static relationship between the position of the robot base and the HoloLens mounted on the robot's head and then store it in a calibration file. Navigation Mode This is the operational mode for navigation. In this mode all the nodes are running. On the Hololens, the HoloLensNavigation app connects to the hololens_ros_bridge node running on the Operations PC and constantly updates spatial anchors for the position of the HoloLens device and dynamic changes in the 3D environment. Through the hololens_ros_bridge, the hololens_localizer nodes monitor the spatial anchor for the HoloLens device and use the static calibration file to constantly calculate and update the relative position of the Pepper robot's base. The ROS navigation stack presents a user interface through the built-in RVIZ application. The RVIZ application is configured to dynamically render the simulated robot's position and spatial sensor data on the 2D navigation map. The RVIZ UI is used to manually position the robot on the navigation map to set an initial \"2D Pose Estimate\" (2D position and polar direction). Checking that the set position agrees with dynamic sensor data within a tolerance threshold, the ROS navigation stack becomes prepared to accept goal positions. The RVIZ UI is now used used to set a \"2D Nav Goal\" on the navigation map. When a navigation goal is received, the move_base node links the local_planner and global_planner nodes to steer and drive the robot's wheels toward the goal until the spatial anchor from the hololens_localizer shows that it has arrived at the destination. System Build and Installation The following links provide guides for preparing the system. They include hardware mounting and configuration, software platform prerequisites, and build and installation instructions. Setup Instructions Map Generation Instructions HoloLens Mounting and Pepper Configuration Instructions Calibration and Navigation Operations The following step-by-step procedure provides a guide for performing the calibration process and commanding the Pepper robot to navigate its way around the space represented by the map. The procedure assumes that all of the previous sections' setup and configuration instructions have been completed and a 2D floor map has already been generated and installed in the system . Launch System HoloLens Stack (HoloLens) Launch HoloLens Bridge Start the HoloLensNavigation application from the browser-hosted Device Portal or alternatively launch it from the device's in-visor Start menu (Navigation PC) Launch Pepper's stack The network interface name for the local ROS computer can be found using the terminal command \"ifconfig\" and looking for the name associated with the active IP address. Do not include the colon after the network interface . The pepper ip address for the robot will be verbally reported when the button on the front of the torso and behind the bottom edge of the tablet is tapped. $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface (Navigation PC) Launch HoloLens Stack The hololens ip value is obtained from the address bar in the browser hosting the Device Portal or from the device's in-visor Settings menu $ roslaunch navigation_launcher hololensstack.launch HoloLens_ip:= hololens ip Navigation Stack (Navigation PC) Launch ROS Navigation System $ roslaunch navigation_launcher navstack.launch Dashboard UI (Navigation PC) Launch the Dashboard User Interface $ rosrun holo_nav_dash holo_nav_dash.py Perform Calibration On the Navigation PC, open your favorite web browser and follow these steps to perform an automatic static calibration and save the data in a text file: navigate to http://localhost:8000 Confirm that all the Required ROS Nodes listed in the UI are running Click the \"Auto HoloLens Calibration\" button in the UI Observe the robot move through the calibration poses and then check the Status window to confirm that a calibration file was successfully saved Navigation Operations Once the system is running and configured, it is ready to perform navigation operations within the physical space represented by the installed map. The following steps start and control navigation: - (Navigation PC) Launch ROS RVIZ $ rosrun rviz rviz - RVIZ will launch showing the 2D map file with a simulated model of the Pepper robot. Use the application GUI to load the pepper.rviz rviz configuration file. In the RVIZ GUI, click the 2D Pose Estimate button and set Pepper's inital position and direction on the map by right-clicking and dragging across the map. The initial position is indicated by the mouse-down click and the direction is calculated from the relative positions of the mouse-down and then mouse-up points on the map. The position must be precise enough to ensure the map data is in harmony with live data coming from the robot's spatial sensors. If the position is within the precision threshold, the script will calculate a pose estimate and localize the Pepper model on the map. In the RVIZ GUI, click the 2D Nav Goal button and select a destination goal and direction for the robot on the map in the same manner used to set the initial 2D Pose Estimate. If a valid path can be calculated by the ROS Navigation Stack, the robot will be begin navigation and movement to the goal position. Once arrived or before arriving at the destination, a new 2D Nav Goal can be specified with the RVIZ GUI and the robot will stop moving to the previous goal position and proceed to the new destination.","title":"Home"},{"location":"#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"#hololens-navigation-for-robots","text":"Welcome! The example system in this repository shows how a HoloLens device can be placed on the head of Pepper robot and provide it with a self-calibrating indoor navigation solution. The calibration process is described in detail in the paper: Dynamic Calibration between a Mobile Robot and SLAM Device for Navigation and is demonstrated in the video below (click image to download video) :","title":"HoloLens Navigation for Robots"},{"location":"#who-might-be-interested-in-using-this-repository","text":"The HoloLens Navigation For Robots project is an example system that exposes key technology challenges in helping robots get where they need to be. Its purpose is to support research and education in the field of Indoor Robot Navigation with an instruction guide for building a sample navigation software system that runs on off-the-shelf hardware components and lends itself to modifications and experimentation. Its function is to measure environmental parameters with 3D spatial sensors to compute and calibrate the position of a robot's mobile base relative to a HoloLens device attached to its head to enable the use of the generic ROS indoor navigation solution for service robots. The designation \"service-robot\" refers to a requirement that it is designed to safely operate in the midst of humans, as opposed to an \"industrial-robot\" that usually repeats tasks within a cordoned-off safety zone. The HoloLens Navigation for Robots repository was adapted from a project originally created by interns working for Microsoft Research Asia. We've built on their work and put together this repository and instruction set to help students learn with a fun hand's-on project that provides exposure to key technologies used in service-robotics. Researchers in the field of autonomous navigation can use the repository to support experimentation in the use of intelligent solutions for route-planning and hazard-avoidance. No matter where your interest lies, we hope you find the HoloLens Navigation for Robots repository a fun and easy way to work and learn in the field of robotics.","title":"Who might be interested in using this repository?"},{"location":"#how-it-works","text":"The system is comprised of three primary hardware components that run the software code modules and navigation operations: a Pepper robot, a HoloLens device, networked together with and bridged to the ROS navigation solution running on a Navigation PC. Additionally, a Build PC running Windows 10 is required to compile and deploy this project's sample application onto the HoloLens device. The system operates in one of three modes: - Map Capture - Calibration - Navigation In the map capture mode, the system takes the 3D environment captured by the HoloLens device and converts it into a 2D map image compatible with the built-in ROS navigation solution. In the calibration mode, the system animates the robot and calculates the position of the HoloLens device mounted on the Pepper robot's head relative to the position of the robot's base, which contains the motorized wheels. In the navigation mode, the system uses the dynamic position of the HoloLens device in the navigation space to indicate the position of the robot's mobile base, which is driven by the ROS navigation solution based on plans computed in response to goal points indicated by the user on a 2D map.","title":"How It Works"},{"location":"#hardware-devices","text":"HoloLens (ver. 1 or 2) - battery-powered mobile computer hosting camera and depth sensors used for map capture and localization Navigation PC (Ubuntu 18.04) - x64 PC hosting ROS (Melodic) navigation solution, calibration, and operational scripts Pepper robot - battery-powered semi-humanoid robot capable of indoor locomotion and independent movement of head and body parts Build PC (Windows 10) - x64 PC hosting Visual Studio 2019 solution for building and deploying the HoloLensNavigation UWP app on the HoloLens device","title":"Hardware Devices:"},{"location":"#sample-software-modules","text":"HoloLensSpatialMapping - Universal Windows Platform (UWP) application solution for HoloLens. It contains two projects: HololensSpatialMapping - Uses device sensors to capture and maintain 3D map of local environment HoloLensBridge - Communication link with ROS system running on the PC hololens_localizer - Custom ROS (Melodic) package that computes the local position of the robot base using sensor measurements as the robot moves through calibrated poses and navigates through the environment. During offline calibration, it computes the relative positions of the HoloLens device on the robot's head and the robot's base static_calibration - node calculates the transform offset between the HoloLens device and the Pepper robot's base and stores it in the calibration file dynamic_adjuster - node that monitors the neck joint positions of the robot and adjusts the transform offset between the HoloLens device and the robot base as needed. anchor_localizer - node that provides the calculated dynamic position of the robot mobile base relative to the 2D navigation map hololens_ros_bridge - Custom ROS (Melodic) package providing IP communication between the HoloLens device and the ROS system holo_nav_dash - Custom ROS (Melodic) package providing a local http server and a browser-based operational interface for starting up and monitoring calibration and navigation operations navigation_launcher - Custom ROS (Melodic) package that contains launch scripts for starting up components for the HoloLens stack, the HoloLens Navigation stack, and the ROS Navigation stack","title":"Sample Software Modules:"},{"location":"#other-software-modules","text":"map_server - built-in ROS node that stores and serves data requests from the map asset image_view - built-in ROS node for viewing image topics and provides the image_saver tool for capturing a graphic image file of the map rviz - built-in ROS node for navigation operations move_base - built-in ROS node providing an implementation of an action that, given a pose (position) goal on the map using rviz, will attempt to reach it by sending movement commands to the robot's mobile base global_planner - built-in ROS node that give a current position and a goal postion on a 2D map, calculates a route through obstacles indicated on the map base_local_planner - built-in ROS node calculates the motor control commands to send to the base in order to follow a route calculated by the global planner Pepper Naoqi Stack - Pepper SDK software providing IP communication nodes between the PC and the Pepper robot, as well as a pose_manager node that interprets movement requests into hardware joint motor commands","title":"Other Software Modules"},{"location":"#modes-of-operation","text":"The system operates in one of three modes: map generation, position calibration, and navigation.","title":"Modes of Operation"},{"location":"#map-capture-and-generation-mode","text":"The purpose of this mode is to create a 2D floorplan of the navigable indoor space. First, the HoloLensNavigation application is run on the HoloLens device while the user scans the indoor space to be navigated by methodically moving it throughout the location. The camera and depth sensors in the device are used to capture and store a 3D spatial map of the room and adjacent areas. After capture, the 3D spatial map must be converted to the 2D floor-plan map form used by the ROS navigation solution. The conversion is accomplished by first launching the hololens_ros_bridge, hololens_localizer, map_server, image_view, and rviz ROS nodes on the PC. The point cloud from the HoloLens spatial map is made available to the ROS system through the hololens_ros_bridge. After the ROS nodes are running, rviz is instructed to use the image_view node and the \"2D Pose Estimate\" process to create a cross-section of the HoloLens' spatial map six inches above and parallel to the floor plane and then save it as a 2D point-cloud image in a JPG file. Using any image editing application, the 2D point cloud is then manually cleaned up to produce a ROS-compliant floorplan navigation map image.","title":"Map Capture and Generation Mode"},{"location":"#position-calibration-mode","text":"The purpose of this mode is to create a static calibration file that defines the relative positions of the Hololens device mounted on the Pepper robot's head and the robot's base containing movement wheels and motors. The HoloLensNavigation app connects to the hololens_ros_bridge node running on the Operations PC and provides spatial anchors representing the current position of the HoloLens device within the 3D map space. On the PC, the Dashboard UI is launched with the holo_nav_dash ROS node and a local http server providing access to data from other ROS nodes as well as Naoqi protocol commands that are sent to the Pepper robot. Buttons are provided in the UI that command the Pepper head to move to specific positions and record sensor readings for use in calculating a static relationship between the position of the robot base and the HoloLens mounted on the robot's head and then store it in a calibration file.","title":"Position Calibration Mode"},{"location":"#navigation-mode","text":"This is the operational mode for navigation. In this mode all the nodes are running. On the Hololens, the HoloLensNavigation app connects to the hololens_ros_bridge node running on the Operations PC and constantly updates spatial anchors for the position of the HoloLens device and dynamic changes in the 3D environment. Through the hololens_ros_bridge, the hololens_localizer nodes monitor the spatial anchor for the HoloLens device and use the static calibration file to constantly calculate and update the relative position of the Pepper robot's base. The ROS navigation stack presents a user interface through the built-in RVIZ application. The RVIZ application is configured to dynamically render the simulated robot's position and spatial sensor data on the 2D navigation map. The RVIZ UI is used to manually position the robot on the navigation map to set an initial \"2D Pose Estimate\" (2D position and polar direction). Checking that the set position agrees with dynamic sensor data within a tolerance threshold, the ROS navigation stack becomes prepared to accept goal positions. The RVIZ UI is now used used to set a \"2D Nav Goal\" on the navigation map. When a navigation goal is received, the move_base node links the local_planner and global_planner nodes to steer and drive the robot's wheels toward the goal until the spatial anchor from the hololens_localizer shows that it has arrived at the destination.","title":"Navigation Mode"},{"location":"#system-build-and-installation","text":"The following links provide guides for preparing the system. They include hardware mounting and configuration, software platform prerequisites, and build and installation instructions.","title":"System Build and Installation"},{"location":"#setup-instructions","text":"","title":"Setup Instructions"},{"location":"#map-generation-instructions","text":"","title":"Map Generation Instructions"},{"location":"#hololens-mounting-and-pepper-configuration-instructions","text":"","title":"HoloLens Mounting and Pepper Configuration Instructions"},{"location":"#calibration-and-navigation-operations","text":"The following step-by-step procedure provides a guide for performing the calibration process and commanding the Pepper robot to navigate its way around the space represented by the map. The procedure assumes that all of the previous sections' setup and configuration instructions have been completed and a 2D floor map has already been generated and installed in the system .","title":"Calibration and Navigation Operations"},{"location":"#launch-system","text":"","title":"Launch System"},{"location":"#hololens-stack","text":"(HoloLens) Launch HoloLens Bridge Start the HoloLensNavigation application from the browser-hosted Device Portal or alternatively launch it from the device's in-visor Start menu (Navigation PC) Launch Pepper's stack The network interface name for the local ROS computer can be found using the terminal command \"ifconfig\" and looking for the name associated with the active IP address. Do not include the colon after the network interface . The pepper ip address for the robot will be verbally reported when the button on the front of the torso and behind the bottom edge of the tablet is tapped. $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface (Navigation PC) Launch HoloLens Stack The hololens ip value is obtained from the address bar in the browser hosting the Device Portal or from the device's in-visor Settings menu $ roslaunch navigation_launcher hololensstack.launch HoloLens_ip:= hololens ip","title":"HoloLens Stack"},{"location":"#navigation-stack","text":"(Navigation PC) Launch ROS Navigation System $ roslaunch navigation_launcher navstack.launch","title":"Navigation Stack"},{"location":"#dashboard-ui","text":"(Navigation PC) Launch the Dashboard User Interface $ rosrun holo_nav_dash holo_nav_dash.py","title":"Dashboard UI"},{"location":"#perform-calibration","text":"On the Navigation PC, open your favorite web browser and follow these steps to perform an automatic static calibration and save the data in a text file: navigate to http://localhost:8000 Confirm that all the Required ROS Nodes listed in the UI are running Click the \"Auto HoloLens Calibration\" button in the UI Observe the robot move through the calibration poses and then check the Status window to confirm that a calibration file was successfully saved","title":"Perform Calibration"},{"location":"#navigation-operations","text":"Once the system is running and configured, it is ready to perform navigation operations within the physical space represented by the installed map. The following steps start and control navigation: - (Navigation PC) Launch ROS RVIZ $ rosrun rviz rviz - RVIZ will launch showing the 2D map file with a simulated model of the Pepper robot. Use the application GUI to load the pepper.rviz rviz configuration file. In the RVIZ GUI, click the 2D Pose Estimate button and set Pepper's inital position and direction on the map by right-clicking and dragging across the map. The initial position is indicated by the mouse-down click and the direction is calculated from the relative positions of the mouse-down and then mouse-up points on the map. The position must be precise enough to ensure the map data is in harmony with live data coming from the robot's spatial sensors. If the position is within the precision threshold, the script will calculate a pose estimate and localize the Pepper model on the map. In the RVIZ GUI, click the 2D Nav Goal button and select a destination goal and direction for the robot on the map in the same manner used to set the initial 2D Pose Estimate. If a valid path can be calculated by the ROS Navigation Stack, the robot will be begin navigation and movement to the goal position. Once arrived or before arriving at the destination, a new 2D Nav Goal can be specified with the RVIZ GUI and the robot will stop moving to the previous goal position and proceed to the new destination.","title":"Navigation Operations"},{"location":"CODE_OF_CONDUCT/","text":"Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"CODE_OF_CONDUCT/#microsoft-open-source-code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"SECURITY/","text":"Security Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below. Reporting Security Issues Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs. Preferred Languages We prefer all communications to be in English. Policy Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"SECURITY"},{"location":"SECURITY/#security","text":"Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below.","title":"Security"},{"location":"SECURITY/#reporting-security-issues","text":"Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs.","title":"Reporting Security Issues"},{"location":"SECURITY/#preferred-languages","text":"We prefer all communications to be in English.","title":"Preferred Languages"},{"location":"SECURITY/#policy","text":"Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"Policy"},{"location":"SUPPORT/","text":"TODO: The maintainer of this repo has not yet edited this file REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo. Support How to file issues and get help This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? . Microsoft Support Policy Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#todo-the-maintainer-of-this-repo-has-not-yet-edited-this-file","text":"REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#support","text":"","title":"Support"},{"location":"SUPPORT/#how-to-file-issues-and-get-help","text":"This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? .","title":"How to file issues and get help"},{"location":"SUPPORT/#microsoft-support-policy","text":"Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"Microsoft Support Policy"},{"location":"Setup/CONFIGURATION/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics HoloLens Mounting and Pepper Configuration Instructions After software installation, the following steps are required to configure the system components in order to connect and function with each other. Mount HoloLens on Pepper The HoloLens device can be conveniently secured to the Pepper robot's head using adhesive velcro strips. Assembly Steps: - First, access the Pepper's joint-release key which is stored under the rubber covering near the Emergency Power Off button behind the robot's neck: Use the joint-release key to remove the access-cover on the back of the robot's head by inserting it into the two holes below the cover: Make a head back strap with adhesive velcro by cutting 10-inch strips of both the hook and loop sides and sticking their adhesive sides together. Attach the strap by slipping it between the flexible fan housing and the head-cover support beam: Affix a 2.5 inch and a 3.5 inch adhesive velcro loop pad to the top of the robot's head in the positions shown: Set the HoloLens device over the robot's head and open the head-band adjust band all the way by rotating the knob all the way to the left: Secure the back of the HoloLens device to the back of the robot's head by tightly creating a closed loop with the two-sided Velcro head strap. Make a second mount strap by creating a 2.5 inch two-sided Velcro strip and securing it over the loop surface of both the HoloLens headstrap and rear mounting pad. Make a third mount strap by creating a 3.5 inch two-sided Velcro strip and securing it over the loop surface of both the HoloLens headstrap and front mounting pad and bringing over the hook side of the HoloLens headstrap to the loop side of the same strap: Pepper Configuration Pepper qicli commands can be issued via the Choregraphe application or directly using a secure shell (SSH) terminal. The Pepper robot's IP address can be obtained by tapping the button on the front of the robot's torso and underneath the bottom edge of the tablet mounted on its chest. The address will be spoken by the robot. If a new wireless network needs to be configured, in some cases a wired ethernet connection must be made to the back of the robot's head. An embedded web server in the robot provides a management website at the robot's IP address. Detailed documentation and further resources for the Pepper robot are on this website: https://developer.softbankrobotics.com/pepper-naoqi-25/pepper-documentation Install Choregraph (Optional): If desired, the Pepper robot's native programming and configuration application Choregraphe can be installed on either the Navigation or Build PC from these webpages: https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux https://developer.softbankrobotics.com/pepper-2-5/downloads/pepper-naoqi-25-downloads-windows Tip: If Choregraph fails to start on the Navigation PC (linux), try: sudo ln -sf /usr/lib/x86_64-linux-gnu/libz.so /opt/'Softbank Robotics'/'Choregraphe Suite 2.5'/lib/libz.so.1 Start Pepper with autonomous life disabled If Choregraphe is available, disable the Pepper robot's autonomous life behavior: Connect to Pepper using Choregraph. Click on blue heart icon in upper right corner. Wake Pepper up by clicking on sunshine icon in upper right corner img\\HololensNavigation_Choregraphe_AutonLifeOff.png Otherwise, disable Pepper autonomous life mode using the secure shell ssh : $ ssh nao@ pepper IP nao stop naoqi-bin --disable-life Pepper Naoqi Shell Commands For reference, following are some example commands that can be ran on the Pepper robot over a secure shell session. A complete listing of the NAOqi API is in the Pepper documentation here: https://developer.softbankrobotics.com/pepper-naoqi-25/naoqi-developer-guide/naoqi-apis Wake Pepper up: $ ssh nao@ pepper IP qicli call ALMotion.wakeUp Make Pepper go to sleep: qicli call ALMotion.rest Shut down Pepper ( The connection will close and further control will be lost until the robot is powered back up ): sudo shutdown -h now Get the joint names for the body or a chain: qicli call ALMotion.getBodyNames Body View Pepper's current joint state: qicli call ALMotion.getSummary Change Pepper's head pitch: qicli call ALMotion.setAngles HeadPitch 0.0 0.1 Note: setAngles is a non-blocking call with the following parameters: - names \u2013 The name or names of joints, chains, \u201cBody\u201d, \u201cJointActuators\u201d, \u201cJoints\u201d or \u201cActuators\u201d. - angles \u2013 One or more angles in radians - fractionMaxSpeed \u2013 The fraction of maximum speed to use Valid Pepper joint names can be found here and here , or call getBodyNames for a complete list. Pepper RVIZ Configuration File The configuration file for the Pepper robot will set up ROS nodes for convenient operations as well as present a simulated model of the robot on the navigation map. Location: /opt/ros/melodic/share/naoqi_driver/share/pepper.rviz Miscellaneous The folowing instructions are optional and provide alternative methods to perfom setup and configuration actions. They are intended to be helpful in system modification and/or troubleshooting. Enumerate network interfaces on Ubuntu: The following IP commands can be used on the Navigation PC's console UI to expose values needed for configuration: $ ip l show $ ip a show eno1 Naoqi Command Sequence for Calibration Positions As an alternative to the Dashboard UI, the console UI in the calibration window can be accessed via an SSH terminal to set the head angles directly. In these examples the first value indicates the joint motor, the second value indicates the angle to move to in radians and last value indicates the speed of movement in seconds. move Pepper's head into inital/default pose: ``` $ ssh nao@ qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.1 ``` press space bar to record the initial position move Pepper's head upward: ``` qicli call ALMotion.setAngles \"HeadPitch\" -0.35 0.1 ``` press space bar again to record the new position reset Pepper's head pitch and then rotate to left: ``` qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.7 0.1 ``` press space bar again to record the new position rotate Pepper's head to the right: ``` qicli call ALMotion.setAngles \"HeadYaw\" -0.7 0.1 ``` press space bar again to record the new position press \"c\" key to calibrate reset Pepper's head pitch and rotation: ``` qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.1 ``` Running Individual Processes The following terminal commands will launch the ROS software modules individually. - Pepper ROS full stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface - example: $ roslaunch pepper_bringup pepper_full.launch nao_ip:=10.1.1.202 network_interface:=enp3s0 HoloLens ROS Bridge $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens_ip 1234 example: $ rosrun hololens_ros_bridge hololens_ros_bridge_node 10.1.1.206 1234 ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml HoloLens Anchor Localizer $ rosrun hololens_localizer anchor_localizer Localizer Calibration $ rosrun hololens_localizer static_calibration robot odom frame robot head frame robot base link [calibrationFileName] example: $ rosrun hololens_localizer static_calibration odom Head base_footprint calibrationData.bin Dynamic Adjuster $ rosrun hololens_localizer dynamic_adjuster.py robot foot frame example: $ rosrun hololens_localizer dynamic_adjuster.py HoloLens Security Certificate Installation Build PC The connection that supports compiled software deployments between Microsoft Visual Studio running on the Build PC and the HoloLens device is secured with a PIN exchange during pairing as described above. However, the \"certificate error\" seen in the browser when accessing the HoloLens Device Portal can be fixed by creating a trust relationship with the device. Each HoloLens generates a self-signed certificate for its SSL connection. By default, this certificate is not trusted by your PC's web browser and you may get a \"certificate error\". You can securely connect to your device by downloading this certificate from your HoloLens over USB or a Wi-Fi network you trust and trusting it on your PC. Make sure you are on a secure network (USB or a Wi-Fi network you trust). Download this device's certificate from the \"Security\" page on the Device Portal. Navigate to: https:// /devicepair.htm Open the node for System Preferences Scroll down to Device Security, select the \"Download this device's certificate\" button. Install the certificate in the \"Trusted Root Certification Authorities\" store on your PC. From the Windows menu, type: Manage Computer Certificates and start the applet. Expand the Trusted Root Certification Authority folder. Select the Certificates folder. From the Action menu, select: All Tasks Import... Complete the Certificate Import Wizard, using the certificate file you downloaded from the Device Portal Restart the browser Note: This certificate will only be trusted for this device and the user will have to go through the process again if the device is flashed. Navigation PC On the Navigation PC, certificates are required to enable secure communications when the Navigation PC initiates connections with the Pepper robot and the HoloLens device. Open the Device Portal in the web browser and HoloLens IP into the address bar: Click on the security warning icon in the left end of the address bar: In the call-out, click \"Connection Not Secure\" , then \"More Information\" to open the Page Info menu screen and then click the \"View Certificate\" button: In the Certificate's page, click the link to download the certificate PEM file: Convert and intall the certificates: $ sudo apt-get install ca-certificates -y $ openssl x509 -outform der -in certificate.pem -out certificate.crt $ sudo cp certificate.crt /usr/local/share/ca-certificates $ sudo update-ca-certificates Change or Disable HoloLens Sleep Settings (Optional) For convenience, it may be desired to change or even turn off the HoloLens device's sleep settings to enable prolonged navigation operations. Navigate to the Device Portal by using your HoloLens IP address in a web browser and log in with the username/password set above. - Navigate to Sleep Settings in the System- Preferences - To disable, right-click on the drop-down selector box for the sleep time-out value and select \"Inspect\" . - add \"0\" as a list option, which will translate to never - select the new list option \"0\" Note: This will need to be performed twice to disable sleep for both \"battery\" and \"plugged-in\" settings.","title":"CONFIGURATION"},{"location":"Setup/CONFIGURATION/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/CONFIGURATION/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/CONFIGURATION/#hololens-mounting-and-pepper-configuration-instructions","text":"After software installation, the following steps are required to configure the system components in order to connect and function with each other.","title":"HoloLens Mounting and Pepper Configuration Instructions"},{"location":"Setup/CONFIGURATION/#mount-hololens-on-pepper","text":"The HoloLens device can be conveniently secured to the Pepper robot's head using adhesive velcro strips. Assembly Steps: - First, access the Pepper's joint-release key which is stored under the rubber covering near the Emergency Power Off button behind the robot's neck: Use the joint-release key to remove the access-cover on the back of the robot's head by inserting it into the two holes below the cover: Make a head back strap with adhesive velcro by cutting 10-inch strips of both the hook and loop sides and sticking their adhesive sides together. Attach the strap by slipping it between the flexible fan housing and the head-cover support beam: Affix a 2.5 inch and a 3.5 inch adhesive velcro loop pad to the top of the robot's head in the positions shown: Set the HoloLens device over the robot's head and open the head-band adjust band all the way by rotating the knob all the way to the left: Secure the back of the HoloLens device to the back of the robot's head by tightly creating a closed loop with the two-sided Velcro head strap. Make a second mount strap by creating a 2.5 inch two-sided Velcro strip and securing it over the loop surface of both the HoloLens headstrap and rear mounting pad. Make a third mount strap by creating a 3.5 inch two-sided Velcro strip and securing it over the loop surface of both the HoloLens headstrap and front mounting pad and bringing over the hook side of the HoloLens headstrap to the loop side of the same strap:","title":"Mount HoloLens on Pepper"},{"location":"Setup/CONFIGURATION/#pepper-configuration","text":"Pepper qicli commands can be issued via the Choregraphe application or directly using a secure shell (SSH) terminal. The Pepper robot's IP address can be obtained by tapping the button on the front of the robot's torso and underneath the bottom edge of the tablet mounted on its chest. The address will be spoken by the robot. If a new wireless network needs to be configured, in some cases a wired ethernet connection must be made to the back of the robot's head. An embedded web server in the robot provides a management website at the robot's IP address. Detailed documentation and further resources for the Pepper robot are on this website: https://developer.softbankrobotics.com/pepper-naoqi-25/pepper-documentation","title":"Pepper Configuration"},{"location":"Setup/CONFIGURATION/#install-choregraph-optional","text":"If desired, the Pepper robot's native programming and configuration application Choregraphe can be installed on either the Navigation or Build PC from these webpages: https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux https://developer.softbankrobotics.com/pepper-2-5/downloads/pepper-naoqi-25-downloads-windows Tip: If Choregraph fails to start on the Navigation PC (linux), try: sudo ln -sf /usr/lib/x86_64-linux-gnu/libz.so /opt/'Softbank Robotics'/'Choregraphe Suite 2.5'/lib/libz.so.1","title":"Install Choregraph (Optional):"},{"location":"Setup/CONFIGURATION/#start-pepper-with-autonomous-life-disabled","text":"If Choregraphe is available, disable the Pepper robot's autonomous life behavior: Connect to Pepper using Choregraph. Click on blue heart icon in upper right corner. Wake Pepper up by clicking on sunshine icon in upper right corner img\\HololensNavigation_Choregraphe_AutonLifeOff.png Otherwise, disable Pepper autonomous life mode using the secure shell ssh : $ ssh nao@ pepper IP nao stop naoqi-bin --disable-life","title":"Start Pepper with autonomous life disabled"},{"location":"Setup/CONFIGURATION/#pepper-naoqi-shell-commands","text":"For reference, following are some example commands that can be ran on the Pepper robot over a secure shell session. A complete listing of the NAOqi API is in the Pepper documentation here: https://developer.softbankrobotics.com/pepper-naoqi-25/naoqi-developer-guide/naoqi-apis Wake Pepper up: $ ssh nao@ pepper IP qicli call ALMotion.wakeUp Make Pepper go to sleep: qicli call ALMotion.rest Shut down Pepper ( The connection will close and further control will be lost until the robot is powered back up ): sudo shutdown -h now Get the joint names for the body or a chain: qicli call ALMotion.getBodyNames Body View Pepper's current joint state: qicli call ALMotion.getSummary Change Pepper's head pitch: qicli call ALMotion.setAngles HeadPitch 0.0 0.1 Note: setAngles is a non-blocking call with the following parameters: - names \u2013 The name or names of joints, chains, \u201cBody\u201d, \u201cJointActuators\u201d, \u201cJoints\u201d or \u201cActuators\u201d. - angles \u2013 One or more angles in radians - fractionMaxSpeed \u2013 The fraction of maximum speed to use Valid Pepper joint names can be found here and here , or call getBodyNames for a complete list.","title":"Pepper Naoqi Shell Commands"},{"location":"Setup/CONFIGURATION/#pepper-rviz-configuration-file","text":"The configuration file for the Pepper robot will set up ROS nodes for convenient operations as well as present a simulated model of the robot on the navigation map. Location: /opt/ros/melodic/share/naoqi_driver/share/pepper.rviz","title":"Pepper RVIZ Configuration File"},{"location":"Setup/CONFIGURATION/#miscellaneous","text":"The folowing instructions are optional and provide alternative methods to perfom setup and configuration actions. They are intended to be helpful in system modification and/or troubleshooting.","title":"Miscellaneous"},{"location":"Setup/CONFIGURATION/#enumerate-network-interfaces-on-ubuntu","text":"The following IP commands can be used on the Navigation PC's console UI to expose values needed for configuration: $ ip l show $ ip a show eno1","title":"Enumerate network interfaces on Ubuntu:"},{"location":"Setup/CONFIGURATION/#naoqi-command-sequence-for-calibration-positions","text":"As an alternative to the Dashboard UI, the console UI in the calibration window can be accessed via an SSH terminal to set the head angles directly. In these examples the first value indicates the joint motor, the second value indicates the angle to move to in radians and last value indicates the speed of movement in seconds. move Pepper's head into inital/default pose: ``` $ ssh nao@ qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.1 ``` press space bar to record the initial position move Pepper's head upward: ``` qicli call ALMotion.setAngles \"HeadPitch\" -0.35 0.1 ``` press space bar again to record the new position reset Pepper's head pitch and then rotate to left: ``` qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.7 0.1 ``` press space bar again to record the new position rotate Pepper's head to the right: ``` qicli call ALMotion.setAngles \"HeadYaw\" -0.7 0.1 ``` press space bar again to record the new position press \"c\" key to calibrate reset Pepper's head pitch and rotation: ``` qicli call ALMotion.setAngles \"HeadPitch\" 0.0 0.1 qicli call ALMotion.setAngles \"HeadYaw\" 0.0 0.1 ```","title":"Naoqi Command Sequence for Calibration Positions"},{"location":"Setup/CONFIGURATION/#running-individual-processes","text":"The following terminal commands will launch the ROS software modules individually. - Pepper ROS full stack $ roslaunch pepper_bringup pepper_full.launch nao_ip:= pepper ip network_interface:= network interface - example: $ roslaunch pepper_bringup pepper_full.launch nao_ip:=10.1.1.202 network_interface:=enp3s0 HoloLens ROS Bridge $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens_ip 1234 example: $ rosrun hololens_ros_bridge hololens_ros_bridge_node 10.1.1.206 1234 ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml HoloLens Anchor Localizer $ rosrun hololens_localizer anchor_localizer Localizer Calibration $ rosrun hololens_localizer static_calibration robot odom frame robot head frame robot base link [calibrationFileName] example: $ rosrun hololens_localizer static_calibration odom Head base_footprint calibrationData.bin Dynamic Adjuster $ rosrun hololens_localizer dynamic_adjuster.py robot foot frame example: $ rosrun hololens_localizer dynamic_adjuster.py","title":"Running Individual Processes"},{"location":"Setup/CONFIGURATION/#hololens-security-certificate-installation","text":"","title":"HoloLens Security Certificate Installation"},{"location":"Setup/CONFIGURATION/#build-pc","text":"The connection that supports compiled software deployments between Microsoft Visual Studio running on the Build PC and the HoloLens device is secured with a PIN exchange during pairing as described above. However, the \"certificate error\" seen in the browser when accessing the HoloLens Device Portal can be fixed by creating a trust relationship with the device. Each HoloLens generates a self-signed certificate for its SSL connection. By default, this certificate is not trusted by your PC's web browser and you may get a \"certificate error\". You can securely connect to your device by downloading this certificate from your HoloLens over USB or a Wi-Fi network you trust and trusting it on your PC. Make sure you are on a secure network (USB or a Wi-Fi network you trust). Download this device's certificate from the \"Security\" page on the Device Portal. Navigate to: https:// /devicepair.htm Open the node for System Preferences Scroll down to Device Security, select the \"Download this device's certificate\" button. Install the certificate in the \"Trusted Root Certification Authorities\" store on your PC. From the Windows menu, type: Manage Computer Certificates and start the applet. Expand the Trusted Root Certification Authority folder. Select the Certificates folder. From the Action menu, select: All Tasks Import... Complete the Certificate Import Wizard, using the certificate file you downloaded from the Device Portal Restart the browser Note: This certificate will only be trusted for this device and the user will have to go through the process again if the device is flashed.","title":"Build PC"},{"location":"Setup/CONFIGURATION/#navigation-pc","text":"On the Navigation PC, certificates are required to enable secure communications when the Navigation PC initiates connections with the Pepper robot and the HoloLens device. Open the Device Portal in the web browser and HoloLens IP into the address bar: Click on the security warning icon in the left end of the address bar: In the call-out, click \"Connection Not Secure\" , then \"More Information\" to open the Page Info menu screen and then click the \"View Certificate\" button: In the Certificate's page, click the link to download the certificate PEM file: Convert and intall the certificates: $ sudo apt-get install ca-certificates -y $ openssl x509 -outform der -in certificate.pem -out certificate.crt $ sudo cp certificate.crt /usr/local/share/ca-certificates $ sudo update-ca-certificates","title":"Navigation PC"},{"location":"Setup/CONFIGURATION/#change-or-disable-hololens-sleep-settings-optional","text":"For convenience, it may be desired to change or even turn off the HoloLens device's sleep settings to enable prolonged navigation operations. Navigate to the Device Portal by using your HoloLens IP address in a web browser and log in with the username/password set above. - Navigate to Sleep Settings in the System- Preferences - To disable, right-click on the drop-down selector box for the sleep time-out value and select \"Inspect\" . - add \"0\" as a list option, which will translate to never - select the new list option \"0\" Note: This will need to be performed twice to disable sleep for both \"battery\" and \"plugged-in\" settings.","title":"Change or Disable HoloLens Sleep Settings (Optional)"},{"location":"Setup/MAP_GENERATION/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Map Generation Instructions HoloLens Spatial Mapping The following step-by-step procedure provides a guide for capturing a 3D spatial map of the navigation space and processing it into a 2D navigation map compatible with the ROS navigation solution. The procedure assumes that all of the previous sections' setup and configuration instructions have been completed and a 2D floor map has already been generated and installed in the system . Use the HoloLens device to perform a spatial mapping of the navigation environment. Map the floor up to at least your eye level and make sure to carefully trace along the floor edges to collect accurate readings. For visual feedback of the map as it builds, compile and run the HoloLensNavigation application and complete the spatial mesh mapping. Alternatively, compile and run the lighter weight Microsoft's Holographic spatial mapping sample at this link . Create a floor plan image from HoloLens' Spatial Map in ROS launch the HoloLensNavigation app on your HoloLens device there are different ways to launch the application. Easiest way is to use the Device Portal to launch application. Alternatively wear the headset and launch from GUI, or launch from VisualStudio. launch the ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml launch the HoloLens bridge in ROS $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens ip 1234 note that 1234 is the port number. launch the HoloLens localizer in ROS $ rosrun hololens_localizer anchor_localizer launch the image saver node in ROS $ rosrun image_view image_saver image:=/hololens/image launch rviz in ROS $ rviz select \"2D Pose Estimate\" and then click anywhere on the map view to set initial position in any location and direction. This instructs image_saver to create a cross-section of the HoloLens' spatial map and save it as left000.jpg in the same folder the image_saver node was launched from. open the output graphic file and edit for navigation open the file left0000.jpg ` with your favorite image editor. It will appear as a pattern of dots filtered by the 2D cross section of the 3D point cloud using the depicted pointcloud outline, create a ROS compliant image map. save the modified left0000.jpg file in the folder: ~/catkin_ws/src/navigation_launcher/params/","title":"MAP GENERATION"},{"location":"Setup/MAP_GENERATION/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/MAP_GENERATION/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/MAP_GENERATION/#map-generation-instructions","text":"","title":"Map Generation Instructions"},{"location":"Setup/MAP_GENERATION/#hololens-spatial-mapping","text":"The following step-by-step procedure provides a guide for capturing a 3D spatial map of the navigation space and processing it into a 2D navigation map compatible with the ROS navigation solution. The procedure assumes that all of the previous sections' setup and configuration instructions have been completed and a 2D floor map has already been generated and installed in the system . Use the HoloLens device to perform a spatial mapping of the navigation environment. Map the floor up to at least your eye level and make sure to carefully trace along the floor edges to collect accurate readings. For visual feedback of the map as it builds, compile and run the HoloLensNavigation application and complete the spatial mesh mapping. Alternatively, compile and run the lighter weight Microsoft's Holographic spatial mapping sample at this link .","title":"HoloLens Spatial Mapping"},{"location":"Setup/MAP_GENERATION/#create-a-floor-plan-image-from-hololens-spatial-map-in-ros","text":"launch the HoloLensNavigation app on your HoloLens device there are different ways to launch the application. Easiest way is to use the Device Portal to launch application. Alternatively wear the headset and launch from GUI, or launch from VisualStudio. launch the ROS map_server $ rosrun map_server map_server src/navigation_launcher/params/map.yaml launch the HoloLens bridge in ROS $ rosrun hololens_ros_bridge hololens_ros_bridge_node hololens ip 1234 note that 1234 is the port number. launch the HoloLens localizer in ROS $ rosrun hololens_localizer anchor_localizer launch the image saver node in ROS $ rosrun image_view image_saver image:=/hololens/image launch rviz in ROS $ rviz select \"2D Pose Estimate\" and then click anywhere on the map view to set initial position in any location and direction. This instructs image_saver to create a cross-section of the HoloLens' spatial map and save it as left000.jpg in the same folder the image_saver node was launched from. open the output graphic file and edit for navigation open the file left0000.jpg ` with your favorite image editor. It will appear as a pattern of dots filtered by the 2D cross section of the 3D point cloud using the depicted pointcloud outline, create a ROS compliant image map. save the modified left0000.jpg file in the folder: ~/catkin_ws/src/navigation_launcher/params/","title":"Create a floor plan image from HoloLens' Spatial Map in ROS"},{"location":"Setup/SETUP/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Setup and Build Instructions This page provides software installation, build, and configuration instructions for the HoloLensNavigationForRobots hardware components including the Navigation PC, the Build PC, the Pepper Robot, and the HoloLens device. Navigation PC This section is a guide for the software required to be installed, built and configured on the Navigation PC. OS and Pre-requisites To match the platform we test with, prepare an x64 Navigation PC meeting the minimum hardware requirements of the Ubuntu 18.04.5 LTS operating system as instructed at: https://releases.ubuntu.com/18.04/ We recommend using the \"standard download\" desktop image for 64-bit PC (AMD64) computers ( ubuntu-18.04.5-desktop-amd64.iso ). This distribution includes a working configuration of Python 2.7 for ROS Melodic and a useful selection of network tools and desktop applications. Install XTerm to host the terminal console-controlled calibration application: $ sudo apt install xterm Install dos2unix. This may be required in some cases to remove DOS carriage return characters from script text files: $ sudo apt install dos2unix Install required Python modules: $ sudo apt install python-pip $ pip install flask $ pip install gevent_websocket ROS Melodic Follow all of the installation instructions on the following web page for ROS Melodic on Ubuntu 18.04: http://wiki.ros.org/melodic/Installation/Ubuntu After a successful initial installation, these additional ROS packages are required: $ sudo apt-get install ros-melodic-driver-base $ sudo apt-get install ros-melodic-move-base-msgs ros-melodic-octomap ros-melodic-octomap-msgs $ sudo apt-get install ros-melodic-map-server $ sudo apt-get install ros-melodic-camera-info-manager ros-melodic-camera-info-manager-py $ sudo apt-get install ros-melodic-rgbd-launch $ sudo apt-get install ros-melodic-husky-navigation $ sudo apt-get install python-catkin-tools Optional tool for manually driving the robot: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-teleop/teleop_twist_keyboard Verify the ROS installation builds with no errors: $ cd ~/catkin_ws/ $ catkin_make Ceres Solver The Ceres Solver provides Python libraries required for computing pathways through the navigation space. http://ceres-solver.org First, install library dependecies: $ sudo apt-get install libgoogle-glog-dev $ sudo apt-get install libatlas-base-dev $ sudo apt-get install libeigen3-dev Then, get the Ceres Solver source code, build and install it: $ mkdir -p ~/ceres $ cd ~/ceres/ $ wget http://ceres-solver.org/ceres-solver-1.14.0.tar.gz $ tar xvf ceres-solver-1.14.0.tar.gz $ mkdir ceres-build cd ceres-build $ cmake ../ceres-solver-1.14.0 $ make -j3 $ sudo make install Pepper Naoqi SDK For reference, general ROS support for the Pepper robot and the Naoqi driver is documented here: http://wiki.ros.org/pepper First, install the following ROS packages for Naoqi: $ sudo apt-get install ros-melodic-pepper-.* $ sudo apt install ros-melodic-naoqi-bridge-msgs ros-melodic-naoqi-libqi ros-melodic-naoqi-driver ros-melodic-naoqi-libqicore Then, download and install the Pepper Naoqi Python SDK: https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux Using the desktop GUI File Manager (or your favorite Linux tool), create a new folder ~/nao as a destination and extract the file pynaoqi-python2.7-2.5.7.1-linux64.tar.gz Add the Naoqi pythonSDK path to the PYTHONPATH in the .bashrc default shell configuration file: $ echo export PYTHONPATH=${PYTHONPATH}:~/nao/pynaoqi-python2.7-2.5.7.1-linux64/lib/python2.7/site-packages ~/.bashrc Clone the source code packages: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-naoqi/naoqi_dcm_driver $ git clone https://github.com/ros-naoqi/naoqi_bridge $ git clone https://github.com/ros-naoqi/pepper_robot $ cd ~/catkin_ws/ $ catkin_make Disable audio in boot configuration file (set flag in line 85 from true to false) using the gedit text-editor application: $ sudo gedit /opt/ros/melodic/share/naoqi_driver/share/boot_config.json Install Hololens Navigation Project Sample Software Using the desktop GUI File Manager (or your favorite Linux tool), download and copy the project sample software for Linux (/linux folder in the repository tree) into the ROS catkin build system: Copy HoloLensNavigationForRobots/linux/hololens_ros_bridge to ~/catkin_ws/src/hololens_ros_bridge Copy HoloLensNavigationForRobots/linux/hololens_localization to ~/catkin_ws/src/hololens_localization Copy HoloLensNavigationForRobots/linux/navigation_launcher to ~/catkin_ws/src/navigation_launcher If needed, the following commands can be used as an example: $ cd ~ $ mkdir repos $ cd repos $ git clone https://github.com/microsoft/HoloLensNavigationForRobots $ cp -r ~/repos/HoloLensNavigationForRobots/linux/hololens_ros_bridge/ ~/catkin_ws/src/hololens_ros_bridge/ $ cp -r ~/repos/HoloLensNavigationForRobots/linux/hololens_localization/ ~/catkin_ws/src/hololens_localization/ $ cp -r ~/repos/HoloLensNavigationForRobots/linux/navigation_launcher/ ~/catkin_ws/src/navigation_launcher/ Update ownership and executable-file permissions to allow the Python scripts to run: $ cd ~/catkin_ws/src/holoLens_localization/scripts $ chown $USER:$USER dynamic_adjuster.py $ chmod +x dynamic_adjuster.py $ chown $USER:$USER localizer.py $ chmod +x localizer.py Make with catkin to build the packages: $ cd ~/catkin_ws/ $ catkin_make Source the workspace: $ . ~/catkin_ws/devel/setup.bash For convenience, all new bash terminals can be set up with this workspace path sourced by default with the following commands: $ echo source /catkin_ws/devel/setup.bash ~/.bashrc $ source ~/.bashrc HoloLens Device This section is a guide for configuring the HoloLens device to run the project sample software. It assumes that the user is already trained and familiar with basic HoloLens UI operations. If the HoloLens device has been set up to require a user-account to log in, this must be done each time before using the project sample software and prior to mounting the Hololens device on the Pepper robot. If the configured user session logs out (ie. for a power-saving time-out), it is required to log back in before continuing to use this software. HoloLens Development Configuration The following configuration settings support development on the HoloLens device. - Settings/Update/For developers/Developer Mode , enabling this setting allows the HoloLens device to run non-store and non-signed applications. - Settings/Update/For developers/Pair , this control sets up a secure pairing with the Build PC to support Microsoft Visual Studio deployments of the compiled sample software on the HoloLens device. - Settings/Update/For developers/Device Portal , enabling this setting launches a web-server on the HoloLens device providing remote browser-based access to platform tools and application-management controls. Full instructions are detailed on this webpage: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-the-windows-device-portal HoloLens Device Portal On the Build PC running Windows, use a browser to navigate to the HoloLens Device Portal by using the IP address of the HoloLens device. Browser security errors will appear. Later, you can install certificates from the device as instructed below, but for this first session click the Advanced button to proceed: Click the \"Continue to [IP address] (unsafe)\" link: The first time the portal is accessed, you will be required to set up a username and password as instructed. The Views/Apps page should will look something like this: Build PC This section is a guide for the software required to be installed, built and configured on the Build PC. OS and Pre-requisites To match the platform we test with, prepare an x64 Build PC meeting the minimum hardware requirements of the Microsoft Windows 10 operating system. Visual Studio 2019 Microsoft Visual Studio 2019 is required to build and deploy the sample HoloLensSpatialMapping application used on the HoloLens device. Download and install the free community version (at minimum) following the instructions on the following website and selecting options to support the Universal Windows Platform build environment : https://visualstudio.microsoft.com/downloads/ HoloLens Spatial Mapping application (Windows) The Visual Studio solution file has all the project dependencies configured to make the system ready to build and deploy the application MSRHoloLensSpatialMapping onto the HoloLens device. Clone this repository with your favorite git tools or download and extract the the files into a convenient folder on the Build PC. From the windows subfolder in the repository, open the file: MSRHoloLensSpatialMapping.sln After the first-time load of the project solution, check that the required linear algebra package Eigen was successfully installed: Navigate to Tools - NuGet Package Manager - Package Manager Console It should appear something like the following: However, if you see the following message in the console: Some NuGet packages are missing from this solution... Fix it by clicking on the Restore button that appears in the UI to download and install the missing Eigen v3.3.3 package. Build and deploy the application to the HoloLens device by clicking the green triangle build button or selecting Build - Build Solution from the menu. For HoloLens 1: Build a \"Release\" solution configuration for \"x86\" platform. - If the device is connected via USB, choose \"Device\" - If the HoloLens is connected via Wifi, choose \"Remote Machine\" For HoloLens 2: Build a \"Release\" solution configuration for \"ARM64\" platform. - If the device is connected via USB, choose \"Device\" - If the HoloLens is connected via Wifi, choose \"Remote Machine\" Full deployment instructions are detailed on this webpage: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-visual-studio After launching the application in the visor, a visible mesh of the immediate environment will begin to fill in as the device is moved through the area. - air-tap to toggle between wirefreame and solid model render mode","title":"SETUP"},{"location":"Setup/SETUP/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"Setup/SETUP/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"Setup/SETUP/#setup-and-build-instructions","text":"This page provides software installation, build, and configuration instructions for the HoloLensNavigationForRobots hardware components including the Navigation PC, the Build PC, the Pepper Robot, and the HoloLens device.","title":"Setup and Build Instructions"},{"location":"Setup/SETUP/#navigation-pc","text":"This section is a guide for the software required to be installed, built and configured on the Navigation PC.","title":"Navigation PC"},{"location":"Setup/SETUP/#os-and-pre-requisites","text":"To match the platform we test with, prepare an x64 Navigation PC meeting the minimum hardware requirements of the Ubuntu 18.04.5 LTS operating system as instructed at: https://releases.ubuntu.com/18.04/ We recommend using the \"standard download\" desktop image for 64-bit PC (AMD64) computers ( ubuntu-18.04.5-desktop-amd64.iso ). This distribution includes a working configuration of Python 2.7 for ROS Melodic and a useful selection of network tools and desktop applications. Install XTerm to host the terminal console-controlled calibration application: $ sudo apt install xterm Install dos2unix. This may be required in some cases to remove DOS carriage return characters from script text files: $ sudo apt install dos2unix Install required Python modules: $ sudo apt install python-pip $ pip install flask $ pip install gevent_websocket","title":"OS and Pre-requisites"},{"location":"Setup/SETUP/#ros-melodic","text":"Follow all of the installation instructions on the following web page for ROS Melodic on Ubuntu 18.04: http://wiki.ros.org/melodic/Installation/Ubuntu After a successful initial installation, these additional ROS packages are required: $ sudo apt-get install ros-melodic-driver-base $ sudo apt-get install ros-melodic-move-base-msgs ros-melodic-octomap ros-melodic-octomap-msgs $ sudo apt-get install ros-melodic-map-server $ sudo apt-get install ros-melodic-camera-info-manager ros-melodic-camera-info-manager-py $ sudo apt-get install ros-melodic-rgbd-launch $ sudo apt-get install ros-melodic-husky-navigation $ sudo apt-get install python-catkin-tools Optional tool for manually driving the robot: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-teleop/teleop_twist_keyboard Verify the ROS installation builds with no errors: $ cd ~/catkin_ws/ $ catkin_make","title":"ROS Melodic"},{"location":"Setup/SETUP/#ceres-solver","text":"The Ceres Solver provides Python libraries required for computing pathways through the navigation space. http://ceres-solver.org First, install library dependecies: $ sudo apt-get install libgoogle-glog-dev $ sudo apt-get install libatlas-base-dev $ sudo apt-get install libeigen3-dev Then, get the Ceres Solver source code, build and install it: $ mkdir -p ~/ceres $ cd ~/ceres/ $ wget http://ceres-solver.org/ceres-solver-1.14.0.tar.gz $ tar xvf ceres-solver-1.14.0.tar.gz $ mkdir ceres-build cd ceres-build $ cmake ../ceres-solver-1.14.0 $ make -j3 $ sudo make install","title":"Ceres Solver"},{"location":"Setup/SETUP/#pepper-naoqi-sdk","text":"For reference, general ROS support for the Pepper robot and the Naoqi driver is documented here: http://wiki.ros.org/pepper First, install the following ROS packages for Naoqi: $ sudo apt-get install ros-melodic-pepper-.* $ sudo apt install ros-melodic-naoqi-bridge-msgs ros-melodic-naoqi-libqi ros-melodic-naoqi-driver ros-melodic-naoqi-libqicore Then, download and install the Pepper Naoqi Python SDK: https://developer.softbankrobotics.com/pepper-naoqi-25-downloads-linux Using the desktop GUI File Manager (or your favorite Linux tool), create a new folder ~/nao as a destination and extract the file pynaoqi-python2.7-2.5.7.1-linux64.tar.gz Add the Naoqi pythonSDK path to the PYTHONPATH in the .bashrc default shell configuration file: $ echo export PYTHONPATH=${PYTHONPATH}:~/nao/pynaoqi-python2.7-2.5.7.1-linux64/lib/python2.7/site-packages ~/.bashrc Clone the source code packages: $ cd ~/catkin_ws/src/ $ git clone https://github.com/ros-naoqi/naoqi_dcm_driver $ git clone https://github.com/ros-naoqi/naoqi_bridge $ git clone https://github.com/ros-naoqi/pepper_robot $ cd ~/catkin_ws/ $ catkin_make Disable audio in boot configuration file (set flag in line 85 from true to false) using the gedit text-editor application: $ sudo gedit /opt/ros/melodic/share/naoqi_driver/share/boot_config.json","title":"Pepper Naoqi SDK"},{"location":"Setup/SETUP/#install-hololens-navigation-project-sample-software","text":"Using the desktop GUI File Manager (or your favorite Linux tool), download and copy the project sample software for Linux (/linux folder in the repository tree) into the ROS catkin build system: Copy HoloLensNavigationForRobots/linux/hololens_ros_bridge to ~/catkin_ws/src/hololens_ros_bridge Copy HoloLensNavigationForRobots/linux/hololens_localization to ~/catkin_ws/src/hololens_localization Copy HoloLensNavigationForRobots/linux/navigation_launcher to ~/catkin_ws/src/navigation_launcher If needed, the following commands can be used as an example: $ cd ~ $ mkdir repos $ cd repos $ git clone https://github.com/microsoft/HoloLensNavigationForRobots $ cp -r ~/repos/HoloLensNavigationForRobots/linux/hololens_ros_bridge/ ~/catkin_ws/src/hololens_ros_bridge/ $ cp -r ~/repos/HoloLensNavigationForRobots/linux/hololens_localization/ ~/catkin_ws/src/hololens_localization/ $ cp -r ~/repos/HoloLensNavigationForRobots/linux/navigation_launcher/ ~/catkin_ws/src/navigation_launcher/ Update ownership and executable-file permissions to allow the Python scripts to run: $ cd ~/catkin_ws/src/holoLens_localization/scripts $ chown $USER:$USER dynamic_adjuster.py $ chmod +x dynamic_adjuster.py $ chown $USER:$USER localizer.py $ chmod +x localizer.py Make with catkin to build the packages: $ cd ~/catkin_ws/ $ catkin_make Source the workspace: $ . ~/catkin_ws/devel/setup.bash For convenience, all new bash terminals can be set up with this workspace path sourced by default with the following commands: $ echo source /catkin_ws/devel/setup.bash ~/.bashrc $ source ~/.bashrc","title":"Install Hololens Navigation Project Sample Software"},{"location":"Setup/SETUP/#hololens-device","text":"This section is a guide for configuring the HoloLens device to run the project sample software. It assumes that the user is already trained and familiar with basic HoloLens UI operations. If the HoloLens device has been set up to require a user-account to log in, this must be done each time before using the project sample software and prior to mounting the Hololens device on the Pepper robot. If the configured user session logs out (ie. for a power-saving time-out), it is required to log back in before continuing to use this software.","title":"HoloLens Device"},{"location":"Setup/SETUP/#hololens-development-configuration","text":"The following configuration settings support development on the HoloLens device. - Settings/Update/For developers/Developer Mode , enabling this setting allows the HoloLens device to run non-store and non-signed applications. - Settings/Update/For developers/Pair , this control sets up a secure pairing with the Build PC to support Microsoft Visual Studio deployments of the compiled sample software on the HoloLens device. - Settings/Update/For developers/Device Portal , enabling this setting launches a web-server on the HoloLens device providing remote browser-based access to platform tools and application-management controls. Full instructions are detailed on this webpage: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-the-windows-device-portal","title":"HoloLens Development Configuration"},{"location":"Setup/SETUP/#hololens-device-portal","text":"On the Build PC running Windows, use a browser to navigate to the HoloLens Device Portal by using the IP address of the HoloLens device. Browser security errors will appear. Later, you can install certificates from the device as instructed below, but for this first session click the Advanced button to proceed: Click the \"Continue to [IP address] (unsafe)\" link: The first time the portal is accessed, you will be required to set up a username and password as instructed. The Views/Apps page should will look something like this:","title":"HoloLens Device Portal"},{"location":"Setup/SETUP/#build-pc","text":"This section is a guide for the software required to be installed, built and configured on the Build PC.","title":"Build PC"},{"location":"Setup/SETUP/#os-and-pre-requisites_1","text":"To match the platform we test with, prepare an x64 Build PC meeting the minimum hardware requirements of the Microsoft Windows 10 operating system.","title":"OS and Pre-requisites"},{"location":"Setup/SETUP/#visual-studio-2019","text":"Microsoft Visual Studio 2019 is required to build and deploy the sample HoloLensSpatialMapping application used on the HoloLens device. Download and install the free community version (at minimum) following the instructions on the following website and selecting options to support the Universal Windows Platform build environment : https://visualstudio.microsoft.com/downloads/","title":"Visual Studio 2019"},{"location":"Setup/SETUP/#hololens-spatial-mapping-application-windows","text":"The Visual Studio solution file has all the project dependencies configured to make the system ready to build and deploy the application MSRHoloLensSpatialMapping onto the HoloLens device. Clone this repository with your favorite git tools or download and extract the the files into a convenient folder on the Build PC. From the windows subfolder in the repository, open the file: MSRHoloLensSpatialMapping.sln After the first-time load of the project solution, check that the required linear algebra package Eigen was successfully installed: Navigate to Tools - NuGet Package Manager - Package Manager Console It should appear something like the following: However, if you see the following message in the console: Some NuGet packages are missing from this solution... Fix it by clicking on the Restore button that appears in the UI to download and install the missing Eigen v3.3.3 package. Build and deploy the application to the HoloLens device by clicking the green triangle build button or selecting Build - Build Solution from the menu. For HoloLens 1: Build a \"Release\" solution configuration for \"x86\" platform. - If the device is connected via USB, choose \"Device\" - If the HoloLens is connected via Wifi, choose \"Remote Machine\" For HoloLens 2: Build a \"Release\" solution configuration for \"ARM64\" platform. - If the device is connected via USB, choose \"Device\" - If the HoloLens is connected via Wifi, choose \"Remote Machine\" Full deployment instructions are detailed on this webpage: https://docs.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/using-visual-studio After launching the application in the visor, a visible mesh of the immediate environment will begin to fill in as the device is moved through the area. - air-tap to toggle between wirefreame and solid model render mode","title":"HoloLens Spatial Mapping application (Windows)"}]}